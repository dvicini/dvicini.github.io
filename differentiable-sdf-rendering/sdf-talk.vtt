WEBVTT

1
00:00:01.680 --> 00:00:05.150
Hi, I am Delio Vicini, and I am PhD student at EPFL.

2
00:00:05.150 --> 00:00:08.680
In this talk, I will present our paper on
”Differentiable Signed Distance Function Rendering”.

3
00:00:09.680 --> 00:00:13.380
This is joint work with Sebastien Speierer
and Wenzel Jakob.

4
00:00:13.380 --> 00:00:17.930
Given a scene description consisting of geometry,
textures and material parameters, we can use

5
00:00:17.930 --> 00:00:21.590
a rendering algorithm to simulate how light
interacts with this scene.

6
00:00:21.590 --> 00:00:25.480
The result of this rendering process is a
realistic image.

7
00:00:25.480 --> 00:00:29.289
In this paper, we investigate the inverse
problem of recovering scene parameters given

8
00:00:29.289 --> 00:00:30.640
one or more images.

9
00:00:30.640 --> 00:00:34.590
We could for example try to reconstruct the
shape of the Stanford bunny from a set of

10
00:00:34.590 --> 00:00:36.410
reference images.

11
00:00:36.410 --> 00:00:40.680
Starting from a simple initialization like
the sphere, our goal is to find the geometry

12
00:00:40.680 --> 00:00:44.820
that minimizes the difference between the
rendered images and the reference images.

13
00:00:44.820 --> 00:00:48.200
Here we only show a single viewpoint, but
in practice we usually use multiple reference

14
00:00:48.200 --> 00:00:49.260
images.

15
00:00:49.260 --> 00:00:53.960
We can minimize the difference between rendered
and reference image by using a differentiable

16
00:00:53.960 --> 00:00:54.960
renderer.

17
00:00:54.960 --> 00:00:57.910
A differentiable renderer computes an image
as a differentiable function of the shape

18
00:00:57.910 --> 00:00:58.910
parameters.

19
00:00:58.910 --> 00:01:03.840
We can then minimize the loss by computing
the derivative with respect to the parameters

20
00:01:03.840 --> 00:01:06.140
and running gradient descent.

21
00:01:06.140 --> 00:01:09.430
The optimization eventually recovers a shape
that closely matches the one observed in the

22
00:01:09.430 --> 00:01:11.540
reference images.

23
00:01:11.540 --> 00:01:15.260
To reconstruct 3D shapes, we need to pick
a suitable geometry representation.

24
00:01:15.260 --> 00:01:17.630
In forward rendering we typically use triangle
meshes.

25
00:01:17.630 --> 00:01:21.770
They can be rendered very efficiently, but
their fixed topology is a limitation when

26
00:01:21.770 --> 00:01:24.280
recovering unknown objects.

27
00:01:24.280 --> 00:01:28.390
An alternative is to use a volumetric rendering
model, which can recover objects of arbitrary

28
00:01:28.390 --> 00:01:29.390
topology.

29
00:01:29.390 --> 00:01:33.260
On the flip side, it loses some of the simplicity
of surface rendering.

30
00:01:33.260 --> 00:01:37.619
A third option is to represent surfaces implicitly
as the zero-level set of a function.

31
00:01:37.619 --> 00:01:43.470
A particularly useful type of implicit representations
are signed distance functions.

32
00:01:43.470 --> 00:01:47.720
Implicit surface representations both support
topological changes during optimization and

33
00:01:47.720 --> 00:01:50.619
retain most of the simplicity of surface rendering.

34
00:01:50.619 --> 00:01:54.170
In this project, we focus on differentiable
rendering of surfaces represented by signed

35
00:01:54.170 --> 00:01:55.329
distance functions.

36
00:01:55.329 --> 00:01:59.770
In short, the goal of this paper is to differentiate
physically-based renderings with respect to

37
00:01:59.770 --> 00:02:02.259
the parameters of an SDF.

38
00:02:02.259 --> 00:02:06.030
Many people have worked on using SDFs for
inverse rendering problems.

39
00:02:06.030 --> 00:02:11.079
However, most previous papers have to either
use some form of a silhouette or mask loss,

40
00:02:11.079 --> 00:02:14.689
do not generalize to gradients of shadows
or global illumination effects, or require

41
00:02:14.689 --> 00:02:17.989
meshing of the implicit surface using marching
cubes.

42
00:02:17.989 --> 00:02:21.909
We propose a new differentiable SDF rendering
method.

43
00:02:21.909 --> 00:02:25.879
Our method generalizes to effects such as
shadows and indirect illumination.

44
00:02:25.879 --> 00:02:30.950
Our algorithm does not require any silhouette
information or explicit meshing.

45
00:02:30.950 --> 00:02:35.099
Our method is based on a reparameterization,
and we carefully derive the Jacobians to make

46
00:02:35.099 --> 00:02:37.059
sure we don’t introduce any bias.

47
00:02:37.059 --> 00:02:41.669
Lastly, we show that this method can directly
reconstruct shapes from images using gradient-descent,

48
00:02:41.669 --> 00:02:47.409
without the need for any complex priors or
silhouette losses.

49
00:02:47.409 --> 00:02:50.790
An obvious question at this point is of course
whether we use neural networks for rendering

50
00:02:50.790 --> 00:02:52.029
or representation.

51
00:02:52.029 --> 00:02:53.599
The short answer is that we don’t.

52
00:02:53.599 --> 00:02:57.519
We use physically-based rendering to estimate
shadows and interreflections using Monte Carlo

53
00:02:57.519 --> 00:02:59.159
integration.

54
00:02:59.159 --> 00:03:02.939
As a representation, we use SDFs that are
stored on a dense regular grid.

55
00:03:02.939 --> 00:03:07.249
An alternative would be to model the SDF values
using a neural network.

56
00:03:07.249 --> 00:03:10.690
Fundamentally, the problem of differentiable
SDF rendering is orthogonal to the concrete

57
00:03:10.690 --> 00:03:13.349
choice of the underlying representation.

58
00:03:13.349 --> 00:03:17.449
Our method is general and does not assume
a specific relation of differentiable parameters

59
00:03:17.449 --> 00:03:18.730
to SDF values.

60
00:03:18.730 --> 00:03:23.579
We use a regular grid for simplicity and ease
of implementation.

61
00:03:23.579 --> 00:03:28.129
Our goal is to differentiate pixel values
with respect to the parameters of an SDF.

62
00:03:28.129 --> 00:03:32.239
In this example, we computed the gradients
of the rendered image with respect to a translation

63
00:03:32.239 --> 00:03:33.829
of the foreground shape.

64
00:03:33.829 --> 00:03:37.569
As you can see, translating this shape not
only produces gradients of directly visible

65
00:03:37.569 --> 00:03:41.689
edges, but also of the shadows cast onto the
background plane.

66
00:03:41.689 --> 00:03:46.959
The blue and red values correspond to negative
and positive gradient signs respectively.

67
00:03:46.959 --> 00:03:51.239
This gradient image here was produced by differentiating
a single parameter, but in practice we might

68
00:03:51.239 --> 00:03:54.340
differentiate millions of shape parameters
at once.

69
00:03:54.340 --> 00:03:58.749
The main challenge in computing such gradients
are visibility discontinuities or occlusions.

70
00:03:58.749 --> 00:04:00.931
For example, consider this scene of a bunny
in front of a background.

71
00:04:00.931 --> 00:04:05.889
For a given pixel, we compute its color by
integrating the image contribution function

72
00:04:05.889 --> 00:04:08.029
f(x) over that pixel.

73
00:04:08.029 --> 00:04:11.540
In this example here, the final pixel color
is a blend of the yellow object color and

74
00:04:11.540 --> 00:04:13.029
the white slide background.

75
00:04:13.029 --> 00:04:16.739
Let’s see now what happens when we translate
the object horizontally.

76
00:04:16.739 --> 00:04:20.799
Here, we denote the horizontal translation
by “pi”.

77
00:04:20.799 --> 00:04:23.860
The translation will affect how much of the
pixel is covered by the object and therefore

78
00:04:23.860 --> 00:04:26.259
change the resulting pixel color.

79
00:04:26.259 --> 00:04:29.789
For that reason, we know that the gradient
of the pixel color integral must be non-zero.

80
00:04:29.789 --> 00:04:35.340
However, when we render this object, we approximate
the pixel color by Monte Carlo integration.

81
00:04:35.340 --> 00:04:40.080
This means we sample the integration domain
with random samples x_i and average their

82
00:04:40.080 --> 00:04:42.289
contributions to estimate the pixel color.

83
00:04:42.289 --> 00:04:46.590
There is now a problem if we want to reason
about the change in pixel color due to the

84
00:04:46.590 --> 00:04:47.870
translation.

85
00:04:47.870 --> 00:04:51.520
The translation of the shape will not affect
the Monte Carlo samples.

86
00:04:51.520 --> 00:04:56.129
Therefore, differentiating this Monte Carlo
estimator results in a gradient that is zero.

87
00:04:56.129 --> 00:05:00.849
But this is incorrect, since we previously
established that this gradient has to be nonzero.

88
00:05:00.849 --> 00:05:03.830
The problem is that there is a discontinuity
in the integrand.

89
00:05:03.830 --> 00:05:07.810
The position of the discontuinity depends
on the parameter “pi”.

90
00:05:07.810 --> 00:05:11.599
The motion of the discontinuity is not captured
when naively differentiating individual Monte

91
00:05:11.599 --> 00:05:14.180
Carlo samples.

92
00:05:14.180 --> 00:05:16.389
But just how important are these discontinuities?

93
00:05:16.389 --> 00:05:20.430
Here, we compare the result of ignoring them
to our method, that correctly accounts for

94
00:05:20.430 --> 00:05:21.840
them.

95
00:05:21.840 --> 00:05:26.030
The two gradient images differ on the visible
edges of the surface, but otherwise look quite

96
00:05:26.030 --> 00:05:27.030
similar.

97
00:05:27.030 --> 00:05:31.270
When running an optimization, we quickly see
that ignoring discontinuities can result in

98
00:05:31.270 --> 00:05:34.490
catastrophic failure and the optimization
just diverges.

99
00:05:34.490 --> 00:05:39.110
There are two classes of methods that can
be used to address this issue.

100
00:05:39.110 --> 00:05:43.819
The first one is edge sampling, where additional
samples are explicitly placed on the discontinuities.

101
00:05:43.819 --> 00:05:48.210
This can be challenging to do, as it requires
extra data structures or complex sampling

102
00:05:48.210 --> 00:05:52.199
strategies
The second approach is to reparameterize the

103
00:05:52.199 --> 00:05:53.199
integral.

104
00:05:53.199 --> 00:05:57.110
This is done such that the samples effectively
move with the discontinuity.

105
00:05:57.110 --> 00:06:01.139
This circumvents the problem and allows to
obtain unbiased gradient estimates without

106
00:06:01.139 --> 00:06:02.569
edge sampling.

107
00:06:02.569 --> 00:06:08.219
In this work, we propose a new reparameterization
method that is tailored to SDFs.

108
00:06:08.219 --> 00:06:10.360
To make this idea of reparameterization more
concrete.

109
00:06:10.360 --> 00:06:12.340
let’s look at a toy problem in 1D.

110
00:06:12.340 --> 00:06:17.069
Let’s say we have some continuous function
f(x) that is defined on the real line.

111
00:06:17.069 --> 00:06:23.169
We multiply this function with a discontinuous
step function with parameter-dependent position.

112
00:06:23.169 --> 00:06:28.160
The value of the parameter “pi” controls
the position of the step function.

113
00:06:28.160 --> 00:06:31.520
Our goal is to compute the derivative of the
integral of this function with respect to

114
00:06:31.520 --> 00:06:33.540
the parameter “pi”.

115
00:06:33.540 --> 00:06:38.150
The problem is now that there is a parameter
dependent discontinuity in the integrand.

116
00:06:38.150 --> 00:06:42.380
For this example, we can come up with a very
simple reparameterization that introduces

117
00:06:42.380 --> 00:06:44.259
a new variable “y”.

118
00:06:44.259 --> 00:06:48.230
We can then express the integral as an integration
over the variable “y”.

119
00:06:48.230 --> 00:06:52.210
This new integral has the same value as the
original one, but the discontinuity is now

120
00:06:52.210 --> 00:06:53.660
independent of “pi”.

121
00:06:53.660 --> 00:06:57.620
Conceptually, the gradient with respect to
“pi” will now be captured by evaluating

122
00:06:57.620 --> 00:07:01.379
the function “f” at a position that depends
on “pi”.

123
00:07:01.379 --> 00:07:05.689
Since we eliminated the moving discontinuity,
we can now correctly estimate the gradient

124
00:07:05.689 --> 00:07:10.710
by simply differentiating a Monte Carlo estimator.

125
00:07:10.710 --> 00:07:14.800
To introduce our reparameterization for SDFs,
let’s look at the problem in a simplified

126
00:07:14.800 --> 00:07:16.340
2D setting.

127
00:07:16.340 --> 00:07:22.080
Here we render a 2D SDF to a 1D pixel by tracing
rays through the pixels support from the left.

128
00:07:22.080 --> 00:07:25.330
The parameter “pi” controls the vertical
translation of the yellow disk.

129
00:07:25.330 --> 00:07:29.610
By default, translating this disk will not
affect incident rays.

130
00:07:29.610 --> 00:07:33.919
Moving the object would simply move the ray
intersection locations along the rays.

131
00:07:33.919 --> 00:07:38.811
Our goal is now to build a reparameterization
that makes sampled rays move with the occluder.

132
00:07:38.811 --> 00:07:43.490
We can think of this as evaluating a motion
vector field at the ray intersection location.

133
00:07:43.490 --> 00:07:46.729
In this example here, the rays that hit the
moving foreground object will “move” with

134
00:07:46.729 --> 00:07:50.550
the parameter “pi”, and the ones on the
background object will stay still.

135
00:07:50.550 --> 00:07:55.029
The motion vector field “V” is defined
to describe the surface motion due to the

136
00:07:55.029 --> 00:07:56.620
parameter “pi”.

137
00:07:56.620 --> 00:08:00.969
For SDFs this parameter-dependent motion can
easily be evaluated as the product of the

138
00:08:00.969 --> 00:08:04.199
SDF’s normal and the SDF’s parameter derivative.

139
00:08:04.199 --> 00:08:08.819
Please see the paper for a detailed derivation
and justification.

140
00:08:08.819 --> 00:08:12.430
The previously described approach results
in the motion vector field being evaluated

141
00:08:12.430 --> 00:08:13.919
on this red line.

142
00:08:13.919 --> 00:08:17.729
At first this seems fine, but there is a major
issue with this approach

143
00:08:17.729 --> 00:08:22.309
The discontinuity in the evaluation of “V”
causes the reparameterization itself to be

144
00:08:22.309 --> 00:08:24.020
discontinuous.

145
00:08:24.020 --> 00:08:28.699
This results in incorrect gradients and therefore
this approach won’t work.

146
00:08:28.699 --> 00:08:32.140
This problem was also encountered in prior
work by Bangaru and colleagues.

147
00:08:32.140 --> 00:08:36.639
While their paper addressed rendering of triangle
meshes, the fundamental problem of constructing

148
00:08:36.639 --> 00:08:39.690
a continuous reparameterization is the same.

149
00:08:39.690 --> 00:08:42.320
In their paper they solve the issue as follows.

150
00:08:42.320 --> 00:08:46.639
For each ray, they trace a set of auxiliary
rays that intersect the scene.

151
00:08:46.639 --> 00:08:51.480
They then evaluate the surface motion at each
auxiliary ray, and average the result to obtain

152
00:08:51.480 --> 00:08:53.560
a continuous reparameterization.

153
00:08:53.560 --> 00:08:57.620
Mathematically, this corresponds to applying
a convolution.

154
00:08:57.620 --> 00:09:03.980
This works and can also be applied to SDFs
but tracing all these auxiliary rays is expensive.

155
00:09:03.980 --> 00:09:08.450
In our work, we solve this issue by leveraging
the global structure imposed by the SDF.

156
00:09:08.450 --> 00:09:12.959
Our motion vector field can be evaluated for
points that aren’t on the actual surface.

157
00:09:12.959 --> 00:09:18.180
We build a valid reparameterization by evaluating
the motion along a continuous curve.

158
00:09:18.180 --> 00:09:22.600
As long as this curve touches the surface
on the discontinuities themselves, this will

159
00:09:22.600 --> 00:09:24.520
result in the right gradients.

160
00:09:24.520 --> 00:09:29.230
We can construct these evaluation locations
by building on sphere tracing, which is the

161
00:09:29.230 --> 00:09:31.320
algorithm that is used to intersect rays with
an SDF.

162
00:09:31.320 --> 00:09:36.910
Given a ray, sphere tracing traverses the
space in steps of the size corresponding to

163
00:09:36.910 --> 00:09:39.220
the distance to the nearest surface.

164
00:09:39.220 --> 00:09:43.630
This distance can be trivially obtained by
querying the SDF.

165
00:09:43.630 --> 00:09:47.650
Sphere tracing then repeatedly takes steps
until eventually an intersection with a surface

166
00:09:47.650 --> 00:09:50.130
is reached or the ray escapes the scene.

167
00:09:50.130 --> 00:09:53.889
We can now modify the standard sphere tracing
method as follows

168
00:09:53.889 --> 00:09:57.550
We can take all the intermediate distances
that were encountered along the ray.

169
00:09:57.550 --> 00:10:01.380
We then compute a weighted sum of those distances.

170
00:10:01.380 --> 00:10:03.329
This results in a new distance ”t”.

171
00:10:03.329 --> 00:10:08.970
We design the weights such that as the ray
approaches a discontinuity, the distance ”t”

172
00:10:08.970 --> 00:10:12.450
will converge to the distance to that specific
discontinuity.

173
00:10:12.450 --> 00:10:16.470
So instead naively evaluating the vector field
”V” at the ray intersection,

174
00:10:16.470 --> 00:10:20.160
we now evaluate “V” on a continuous line
of points.

175
00:10:20.160 --> 00:10:23.870
This ensures that our reparameterization is
continuous, which allows to then correctly

176
00:10:23.870 --> 00:10:27.410
compute gradients while accounting for occlusion
effects.

177
00:10:27.410 --> 00:10:29.870
There are a few more aspects that I didn't
have time to cover.

178
00:10:29.870 --> 00:10:32.639
In practice, we need to reparameterize the
spherical integration domain,

179
00:10:32.639 --> 00:10:37.000
and we also need to make this all work for
nested integrals along light paths.

180
00:10:37.000 --> 00:10:39.180
You can find those details in the paper.

181
00:10:39.180 --> 00:10:43.540
We validate the gradients computed using our
method against finite differences on a range

182
00:10:43.540 --> 00:10:44.540
of scene.

183
00:10:44.540 --> 00:10:48.100
We can see that our method closely matches
the reference, even for gradients of shadows

184
00:10:48.100 --> 00:10:51.500
and reflections.

185
00:10:51.500 --> 00:10:55.670
Our method can then be used to reconstruct
geometry directly from images using gradient

186
00:10:55.670 --> 00:10:56.670
descent.

187
00:10:56.670 --> 00:10:59.470
Here the shape of the dragon is recovered
from 12 input images.

188
00:10:59.470 --> 00:11:04.570
We optimize SDFs using the Adam optimizer
and in each iteration enforce the SDF property

189
00:11:04.570 --> 00:11:07.430
by solving the Eikonal equation.

190
00:11:07.430 --> 00:11:11.180
In this more complex example, we reconstruct
shape, texture and roughness of this chair

191
00:11:11.180 --> 00:11:13.070
all simultaneously.

192
00:11:13.070 --> 00:11:16.360
The albedo and roughness variation is represented
by 3D textures.

193
00:11:16.360 --> 00:11:19.720
The result is re-rendered under an illumination
condition that is different from the one used

194
00:11:19.720 --> 00:11:22.230
during optimization.

195
00:11:22.230 --> 00:11:25.709
Our method generalizes to differentiating
secondary effects such as shadows and global

196
00:11:25.709 --> 00:11:26.709
illumination.

197
00:11:26.709 --> 00:11:30.740
It turns out that for scenarios with limited
observations such gradients can really benefit

198
00:11:30.740 --> 00:11:32.550
shape reconstruction.

199
00:11:32.550 --> 00:11:37.370
In this example, we try to reconstruct this
red object from a single view.

200
00:11:37.370 --> 00:11:39.730
In the middle we ignore the gradients of the
shadow.

201
00:11:39.730 --> 00:11:44.560
On the right we use our method to also optimize
the shape based on the shadow that we see.

202
00:11:44.560 --> 00:11:48.730
Ignoring the shadows leads to the optimizer
trying to match the darkness of the background

203
00:11:48.730 --> 00:11:50.920
by placing small surface parts.

204
00:11:50.920 --> 00:11:55.329
When re-rendering the recovered geometry from
a different view, we see that accounting for

205
00:11:55.329 --> 00:11:59.190
secondary gradients allowed us to recover
something that is surprisingly close to the

206
00:11:59.190 --> 00:12:01.630
true shape even from a single view.

207
00:12:01.630 --> 00:12:05.130
On the other hand, ignoring secondary effects
completely fails to recover the true shape

208
00:12:05.130 --> 00:12:11.029
We can observe similar behavior in this scene,
where we try to reconstruct this bunny that

209
00:12:11.029 --> 00:12:13.950
is visible both directly and through a rough
mirror surface.

210
00:12:13.950 --> 00:12:18.470
We again optimize using only a single input
view, and again ignoring the indirect gradients

211
00:12:18.470 --> 00:12:23.820
results in the optimizer not being able to
match the appearance of the object in the

212
00:12:23.820 --> 00:12:24.820
reflection.

213
00:12:24.820 --> 00:12:28.040
We also compare our approach to the reparameterization
introduced by Bangaru and colleagues.

214
00:12:28.040 --> 00:12:32.300
Their method was designed for triangle meshes
but can be adapted to SDFs.

215
00:12:32.300 --> 00:12:36.790
This is the only prior work that can support
physically-based differentiable SDF rendering.

216
00:12:36.790 --> 00:12:41.470
They construct a continuous reparameterization
by applying a convolution over ray directions.

217
00:12:41.470 --> 00:12:47.350
If we use too few samples to evaluate this
convolution, the reconstructed geometry suffers

218
00:12:47.350 --> 00:12:48.750
from artifacts.

219
00:12:48.750 --> 00:12:53.579
On the other hand, higher sample counts lead
to much higher computation times.

220
00:12:53.579 --> 00:12:58.250
Our method both gives accurate results and
runs faster than previous work.

221
00:12:58.250 --> 00:13:02.220
Here we compare both methods on a scene where
we vary the number of reference images.

222
00:13:02.220 --> 00:13:05.660
Our method is more accurate and can therefore
more robustly reconstruct the shape even at

223
00:13:05.660 --> 00:13:07.540
a low number of input views.

224
00:13:07.540 --> 00:13:12.000
In this example here, the difference becomes
a bit smaller as the number of views is increased.

225
00:13:12.000 --> 00:13:16.360
We can also visualize the averaged optimization
results over 8 different sets of reference

226
00:13:16.360 --> 00:13:17.360
images.

227
00:13:17.360 --> 00:13:21.519
The haze in these images is due to different
configurations producing drastically different

228
00:13:21.519 --> 00:13:22.519
results.

229
00:13:22.519 --> 00:13:26.970
Again, our method is more robust and consistent
than the previous approach.

230
00:13:26.970 --> 00:13:31.820
One limitation of our SDF optimization is
that the problem isn’t necessarily convex.

231
00:13:31.820 --> 00:13:35.810
Here we tried to reconstruct this Lego bulldozer
and our method filled in some of the parts

232
00:13:35.810 --> 00:13:37.779
of the shape that should not be filled in.

233
00:13:37.779 --> 00:13:42.040
In these cases, the gradient does not provide
sufficient information to escape this undesirable

234
00:13:42.040 --> 00:13:43.420
local minimum.

235
00:13:43.420 --> 00:13:47.230
This is a limitation of reconstructing shapes
using a differentiable surface rendering method

236
00:13:47.230 --> 00:13:50.839
and doesn’t just affect our method.

237
00:13:50.839 --> 00:13:54.480
In summary, our method is the first general
differentiable SDF rendering approach that

238
00:13:54.480 --> 00:13:56.370
can also differentiate secondary effects.

239
00:13:56.370 --> 00:14:00.040
We leveraged the sphere tracing algorithm
to construct a reparameterization of the domain

240
00:14:00.040 --> 00:14:01.770
that handles discontinuities.

241
00:14:01.770 --> 00:14:06.699
This allows to reconstruct shapes without
a silhouette or mask loss.

242
00:14:06.699 --> 00:14:10.139
For future work, it would be interesting to
try to reduce the issue of undesirable local

243
00:14:10.139 --> 00:14:11.139
minima.

244
00:14:11.139 --> 00:14:15.220
This could be done by initializing using a
volumetric reconstruction or by introducing

245
00:14:15.220 --> 00:14:16.860
semitransparent rendering model.

246
00:14:16.860 --> 00:14:20.910
It would also be interesting to see if we
could apply a similar approach to triangle

247
00:14:20.910 --> 00:14:21.910
meshes.

248
00:14:21.910 --> 00:14:25.620
One could try to build a reparameterization
of a triangle mesh by collecting some extra

249
00:14:25.620 --> 00:14:29.069
information during the BVH traversal in the
ray intersection routine.

250
00:14:29.069 --> 00:14:33.770
Finally, the paper and supplemental video
are available on our website and an open-source

251
00:14:33.770 --> 00:14:36.890
implementation based on Mitsuba 3 can be found
on GitHub.

252
00:14:36.890 --> 00:14:38.299
Thank you for your attention!

